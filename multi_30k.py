# -*- coding: utf-8 -*-
"""Multi-30K.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n3RWBFGtxOwsvLwFCpRD4k8LURpRy1uD
"""

!pip install datasets
import torch
import torch.nn as nn
import torch.nn.functional as f
import torch.optim as optim
import numpy as np
import re
import math
import nltk
import pickle
import matplotlib.pyplot as plt

from nltk import word_tokenize
from torch.utils.data import Dataset
from datasets import load_dataset
from gensim.models import Word2Vec
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
nltk.download('punkt')

dataset = load_dataset("bentrevett/multi30k")

# Embedding
class Embedding(nn.Module):
  def __init__(self,vocab_size,d_model):
    super().__init__()
    self.vocab_size = vocab_size
    self.d_model    = d_model
    self.embedding  = nn.Embedding(self.vocab_size,self.d_model).to(device)

  def forward(self,x):
    x = self.embedding(x)
    return x/math.sqrt(self.d_model)

#MultiHead Attention
class MultiHeadAttention(nn.Module):
  def __init__(self,d_model,heads):
    super().__init__()
    self.d_model = d_model
    self.heads   = heads
    self.d_k     = self.d_model//self.heads

    self.dropout = nn.Dropout(0.5)
    self.w_q     = nn.Linear(self.d_model,self.d_model).to(device)
    self.w_k     = nn.Linear(self.d_model,self.d_model).to(device)
    self.w_v     = nn.Linear(self.d_model,self.d_model).to(device)
    self.w_o     = nn.Linear(self.d_model,self.d_model).to(device)
  def attention(self,q,k,v,drop = True):
    d_k = q.shape[-1]
    attention_scores = q @k.transpose(-2,-1)/math.sqrt(d_k)
    attention_scores = f.softmax(attention_scores)
    if drop:
      attention_scores = self.dropout(attention_scores)
    attention = attention_scores @v
    return attention, attention_scores
  def forward(self,q,k,v):
    query = self.w_q(q)
    key = self.w_k(k)
    value = self.w_v(v)
    attention,score = self.attention(query,key,value)
    attention = attention.contiguous()
    attention = self.dropout(attention)
    return self.w_o(attention)

class ResidualConnection(nn.Module):
  def __init__(self,d_model):
    super().__init__()
    self.d_model = d_model
    self.norm    = nn.LayerNorm(self.d_model).to(device)
    self.dropout = nn.Dropout(0.5)

  def forward(self,x,sublayer):
    x = x + self.dropout(sublayer(x))
    return self.norm(x)

class FeedForward(nn.Module):
  def __init__(self,d_model):
    super().__init__()
    self.d_model = d_model
    self.layer1 = nn.Linear(self.d_model,self.d_model*4).to(device)
    self.layer2 = nn.Linear(self.d_model*4,self.d_model).to(device)
    self.dropout= nn.Dropout(0.5)

  def forward(self,x):
    x = f.relu(self.layer1(x))
    x = self.dropout(self.layer2(x))
    return x

class EncoderBlock(nn.Module):
  def __init__(self,d_model,attention,feedforward):
    super().__init__()
    self.attention    = attention
    self.network      = feedforward
    self.residual     = nn.ModuleList([ResidualConnection(d_model) for _ in range(2)])

  def forward(self,x):
    x = self.residual[0](x, lambda x: self.attention(x,x,x))
    x = self.residual[1](x, lambda x: self.network(x))
    return x

class Encoder(nn.Module):
  def __init__(self,layers):
    super().__init__()
    self.layers = layers
  def forward(self,x):
    for layer in self.layers:
      x = layer(x)
    return x

class Decoder(nn.Module):
  def __init__(self,layers):
    super().__init__()
    self.layers = layers
  def forward(self,encoder,x):
    for layer in self.layers:
      x = layer(encoder,x)
    return x

class DecoderBlock(nn.Module):
  def __init__(self,d_model,attention,feedforward):
    super().__init__()
    self.attention = attention
    self.network   = feedforward
    self.residual     = nn.ModuleList([ResidualConnection(d_model) for _ in range(3)])

  def forward(self,encoder_output,x):
    x = self.residual[0](x , lambda x: self.attention(x,x,x))
    x = self.residual[1](x , lambda x: self.attention(encoder_output,encoder_output,x))
    x = self.residual[2](x , lambda x: self.network(x))
    return x

class ProjectionLayer(nn.Module):
  def __init__(self,d_model,trg_vocab):
    super().__init__()
    self.linear = nn.Linear(d_model,trg_vocab).to(device)

  def forward(self,x):
    x = f.softmax(self.linear(x))
    return x

class TransformerModel(nn.Module):
  def __init__(self,d_model,src_vocab,trg_vocab,layers):
    super().__init__()
    self.d_model = d_model
    self.layers  = layers
    self.src_embed = Embedding(src_vocab,d_model).to(device)
    self.trg_embed = Embedding(trg_vocab,d_model).to(device)
    self.encoder   = Encoder(self.encoderMaker())
    self.decoder   = Decoder(self.decoderMaker())
    self.projection= ProjectionLayer(d_model,trg_vocab).to(device)
    self.init_weights()

  def init_weights(self):
    for m in self.modules():
      if isinstance(m,nn.Embedding):
        nn.init.normal_(m.weight,mean = 0.0,std = 1.0)
      elif isinstance(m,nn.Linear):
        nn.init.xavier_normal_(m.weight)
      elif isinstance(m,nn.LayerNorm):
        nn.init.zeros_(m.weight)

  def encoderMaker(self):
    encoder = nn.ModuleList()
    for i in range(self.layers):
      enc = EncoderBlock(self.d_model,MultiHeadAttention(self.d_model,4),FeedForward(self.d_model))
      encoder.append(enc)
    return encoder.to(device)

  def decoderMaker(self):
    decoder = nn.ModuleList()
    for i in range(self.layers):
      dec = DecoderBlock(self.d_model,MultiHeadAttention(self.d_model,4),FeedForward(self.d_model))
      decoder.append(dec)
    return decoder.to(device)

  def forward(self,src,trg):
    src = self.src_embed(src)
    trg = self.trg_embed(trg)
    encode = self.encoder(src)
    decode = self.decoder(encode,trg)
    pred   = self.projection(decode)
    return pred

# class Transformer(nn.Module):
#   def __init__(self,d_model,encoder,decoder,projection,src_embed,trg_embed):
#     super().__init__()
#     self.d_model = d_model
#     self.encoder = encoder
#     self.decoder = decoder
#     self.projection    = projection
#     self.src_embedding = src_embed
#     self.trg_embedding = trg_embed
#     self.init_weights()
#   def init_weights(self):
#     for m in self.modules():
#       if isinstance(m,nn.Embedding):
#         nn.init.normal_(m.weight,mean = 0.0,std = 1.0)
#       elif isinstance(m,nn.Linear):
#         nn.init.xavier_normal_(m.weight)
#       elif isinstance(m,nn.LayerNorm):
#         nn.init.zeros_(m.weight)
#   def positional_encoding(self,x):
#     pe = torch.zeros(len(x), self.d_model)
#     position = torch.arange(0, len(x), dtype=torch.float).unsqueeze(1)
#     div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * (-math.log(10000.0) / self.d_model))
#     pe[:, 0::2] = torch.sin(position * div_term)
#     pe[:, 1::2] = torch.cos(position * div_term)
#     pe = pe.unsqueeze(0)
#     return pe

#   def forward(self,src,trg):
#     src = self.src_embedding(src)
#     trg = self.trg_embedding(trg)
#     src_pe = self.positional_encoding(src)
#     trg_pe = self.positional_encoding(trg)
#     src = src#+ src_pe
#     trg = trg#+ trg_pe
#     encode = self.encoder(src)
#     decode = self.decoder(encode,trg)
#     pred   = self.projection(decode)
#     return pred

!cp /content/drive/MyDrive/Multi-30k/eng_preprocessing /content/
!cp /content/drive/MyDrive/Multi-30k/ger_preprocessing /content/
!cp /content/drive/MyDrive/Multi-30k/eng_tokenization /content/
!cp /content/drive/MyDrive/Multi-30k/ger_tokenization /content/
!cp /content/drive/MyDrive/Multi-30k/eng_vectroization /content/
!cp /content/drive/MyDrive/Multi-30k/ger_vectroization /content/
# !cp /content/drive/MyDrive/Multi-30k/transformer_30k.pth /content/

class Multi30K(Dataset):
  def __init__(self,data,max,save = False):
    self.data = data
    self.save = save
    self.size = 29000
    self.max  = max
    self.eng  = data['en']
    self.ger  = data['de']
    self.eng_pre,self.ger_pre = self.preprocessing()
    self.eng_tok,self.ger_tok = self.tokenization()
    self.eng_vect,self.ger_vect = self.vectroization()
    self.src_vocab = len(list(self.eng_vect))
    self.trg_vocab = len(list(self.ger_vect))
    self.pad_idx   = 0
    self.sos_idx   = 1
    self.eos_idx   = 2
    self.unk_idx   = 3

  def preprocessing(self):
    if self.save:
      with open(f"eng_preprocessing","rb") as f:
        english = pickle.load(f)
      with open(f"ger_preprocessing","rb") as f:
        germnan = pickle.load(f)
    else:
      pattern = r'[^\w\s]'
      english = []
      germnan = []
      for i in range(len(self.eng)):
        eng = re.sub(pattern,"",self.eng[i].lower())
        ger = re.sub(pattern,"",self.ger[i].lower())
        english.append(eng)
        germnan.append(ger)
        with open("eng_preprocessing","wb") as f:
            pickle.dump(english,f)
        with open("ger_preprocessing","wb") as f:
          pickle.dump(germnan,f)
    return english,germnan

  def tokenization(self):
    if self.save:
      with open(f"eng_tokenization","rb") as f:
        english = pickle.load(f)
      with open(f"ger_tokenization","rb") as f:
        germnan = pickle.load(f)
    else:
      english,germnan = [],[]
      for i in range(self.size):
        english.append(word_tokenize(self.eng_pre[i]))
        germnan.append(word_tokenize(self.ger_pre[i]))
      with open("eng_tokenization","wb") as f:
        pickle.dump(english,f)
      with open("ger_tokenization","wb") as f:
        pickle.dump(germnan,f)
    return english,germnan

  def vectroization(self):
    if self.save:
      with open(f"eng_vectroization","rb") as f:
        english = pickle.load(f)
      with open(f"ger_vectroization","rb") as f:
        german = pickle.load(f)
    else:
      english,german = {},{}
      eng_model = Word2Vec(self.eng_tok,min_count = 1)
      ger_model = Word2Vec(self.ger_tok,min_count = 1)
      eng_vocab = eng_model.wv.key_to_index
      ger_vocab = ger_model.wv.key_to_index
      english.update({"<pad>":0})
      english.update({"<sos>":1})
      english.update({"<eos>":2})
      english.update({"<unk>":3})
      german.update({"<pad>":0})
      german.update({"<sos>":1})
      german.update({"<eos>":2})
      german.update({"<unk>":3})
      print(eng_vocab)
      for idx,word in enumerate(eng_vocab):
        english.update({word:idx+4})
      for idx,word in enumerate(ger_vocab):
        german.update({word:idx+4})

      with open("eng_vectroization","wb") as f:
        pickle.dump(english,f)
      with open("ger_vectroization","wb") as f:
        pickle.dump(german,f)
    return english,german

  def tokenformation(self,eng,ger):
    eng_token = []
    ger_token = []
    eng_pre = re.sub(r'[^\w\s]','',eng)
    ger_pre = re.sub(r'[^\w\s]','',ger)
    eng_tok = word_tokenize(eng_pre)
    ger_tok = word_tokenize(ger_pre)
    for i in range(len(eng_tok)):
      if eng_tok[i] in self.eng_vect:
        eng_token.append(self.eng_vect[eng_tok[i]])
      else:
        eng_token.append(3)

    for i in range(len(ger_tok)):
      if ger_tok[i] in self.ger_vect:
        ger_token.append(self.ger_vect[ger_tok[i]])
      else:
        ger_token.append(3)

    eng_max = self.max - len(eng_token)
    ger_max = self.max - len(ger_token)
    eng_ten = torch.cat([
        torch.tensor([1]),
        torch.tensor(eng_token),
        torch.tensor([2]),
        torch.tensor([0]*eng_max)
    ]).long().to(device)
    ger_ten = torch.cat([
        torch.tensor([1]),
        torch.tensor(ger_token),
        torch.tensor([2]),
        torch.tensor([0]*ger_max)
    ]).long().to(device)
    eng_mask = torch.tensor([1]*(len(eng_token)+2)).to(device)
    ger_mask = torch.tensor([1]*(len(ger_token)+2)).to(device)
    return eng_ten,ger_ten,eng_mask,ger_mask

  def __getitem__(self,index):
    eng,ger = self.eng[index].lower(),self.ger[index].lower()
    eng_ten,ger_ten,eng_mask,ger_mask = self.tokenformation(eng,ger)
    return eng_ten,ger_ten,eng_mask,ger_mask

  def __len__(self):
    return self.size

data      = Multi30K(dataset["train"],56,True)
d_model   = 512
model     = TransformerModel(d_model,data.src_vocab,data.trg_vocab,6).to(device)
optimizer = optim.Adam(model.parameters(),lr = 0.0001)

class Train:
  def __init__(self):
    self.loss = []
    self.loss_fn = nn.CrossEntropyLoss()

  def saveandload(self,idx):
    if idx %500 ==0 and idx!= 0:
      torch.save(model.state_dict(),"transformer_30k.pth")

    if idx%1000==0 and idx!=0:
      model.load_state_dict(torch.load("transformer_30k.pth"))

  def ploting(self):
    plt.plot(self.loss)
    plt.xlabel("episode")
    plt.ylabel("loss")
    plt.title("multi_30k_loss")
    plt.savefig("loss.png")
    plt.close()

  def masking(self,trg,trg_mask,pred):
    trg  = trg[:len(trg_mask)]
    pred = pred[:len(trg_mask)]
    return trg,pred

  def calculate_accuracy(self, outputs, targets):
    correct = (outputs == targets).float().sum()
    total = targets.numel()
    accuracy = correct / total
    return accuracy

  def train(self):
    model.train()
    for i in range(len(data)):
      src,trg,src_mask,trg_mask = data[i]
      pred = model(src,trg)
      output = pred.argmax(1)
      trg,output = self.masking(trg,trg_mask,output)
      loss   = self.loss_fn(output.float(),trg.float())
      # loss   = loss/1000000
      # loss   = torch.clamp(loss,0,250)
      loss.requires_grad = True
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      accurcy = self.calculate_accuracy(output,trg)
      print(f'episode {i} / {len(data)} loss: {loss} completioned: {i/len(data) *100}% accurcy {accurcy}')
      self.loss.append(loss.item())
      self.saveandload(i)
      self.ploting()

  def test(self):
    model.load_state_dict(torch.load("transformer_30k.pth"))
    model.eval()
    for i in range(1000):
      src,trg,src_mask,trg_mask = data[i]
      pred = model(src,trg)
      output = pred.argmax(1)

train = Train()
train.train()